# ğŸ©º Chest X-Ray Pneumonia Detection (Production-Ready ML Project)
Pneumonia is an inflammatory condition of the lung affecting primarily the small air sacs known as alveoli. Symptoms typically include some combination of productive or dry cough, chest pain, fever and difficulty breathing. The severity of the condition is variable. Pneumonia is usually caused by infection with viruses or bacteria and less commonly by other microorganisms, certain medications or conditions such as autoimmune diseases.Risk factors include cystic fibrosis, chronic obstructive pulmonary disease (COPD), asthma, diabetes, heart failure, a history of smoking, a poor ability to cough such as following a stroke and a weak immune system. Diagnosis is often based on symptoms and physical examination. Chest X-ray, blood tests, and culture of the sputum may help confirm the diagnosis.The disease may be classified by where it was acquired, such as community- or hospital-acquired or healthcare-associated pneumonia.

This project implements an **end-to-end, production-ready deep learning pipeline** for detecting **Pneumonia vs Normal** cases from **Chest X-Ray images** using **PyTorch**. The codebase follows **industry-standard MLOps practices**, clean architecture, and clear separation of concerns.

---

## ğŸš€ Project Objective

To build, train, evaluate, and deploy a **binary image classification model** that can accurately distinguish between:

* **NORMAL** chest X-rays
* **PNEUMONIA** chest X-rays

The project is designed to be:

* Modular
* Scalable
* Cloud-ready (AWS S3 + BentoML)
* Production ready

---

![xray_arch](https://github.com/BIKRANT-RAWAT/Lungs-Disease-Diagnosis-CV-DL/blob/main/flowcharts/overall.jpg)

---
## ğŸ“Š Model Visualization (FastAPI)

![interfacel](https://github.com/BIKRANT-RAWAT/Lungs-Disease-Diagnosis-CV-DL/blob/main/images/file_choose.png)
![Prediction](https://github.com/BIKRANT-RAWAT/Lungs-Disease-Diagnosis-CV-DL/blob/main/images/response.png)

---

## ğŸ§  Key Highlights

* âœ… Binary classification (COVID class removed intentionally)
* âœ… Custom CNN architecture in PyTorch
* âœ… Robust data augmentation & normalization
* âœ… Best-model checkpointing
* âœ… Clean separation of **model**, **training**, **inference**, and **artifacts**
* âœ… BentoML-ready for deployment
* âœ… GPU/CPU compatible

---
## ğŸ’¾ Dataset used

The dataset was shared by **Apollo Diagnostic Center** for research purposes. A Proof of Concept (POC) was built using this proprietary dataset to validate the pneumonia detection pipeline.

---

## ğŸ’» Tech Stack Used

1. Python
2. FastAPI
3. PyTorch
4. Docker
5. AWS
6. Azure

---

## ğŸ–¥ Infrastructure Required

1. AWS S3
2. AWS App Runner
3. GitHub Actions

---

## ğŸ¯ How to Run

### Step 1: Download the project

```bash
Download the zip file and extract it to a folder.
or
Run code cell in notebook to download from kaggle.
```

### Step 2: Create a Conda environment

```bash
python -m venv .venv
```

```bash
.venv\Scripts\activate.bat
```

### Step 3: Install dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Export environment variables

```bash
export AWS_ACCESS_KEY_ID=<AWS_ACCESS_KEY_ID>
export AWS_SECRET_ACCESS_KEY=<AWS_SECRET_ACCESS_KEY>
export AWS_DEFAULT_REGION=<AWS_DEFAULT_REGION>
```

### Step 5: Run the application server

```bash
python app.py
```

### Step 6: Train the model

```bash
http://localhost:8001/train
```

### Step 7: Run prediction

```bash
http://localhost:8001/predict
```

---

## ğŸš¢ Run Using Docker (Local)

1. Ensure `Dockerfile` is present in the project root directory.

2. Build the Docker image

```bash
docker build -t xray_classification .
```

3. Run the Docker container

```bash
docker run -d -p 8001:8001 \
  -e AWS_ACCESS_KEY_ID=<AWS_ACCESS_KEY_ID> \
  -e AWS_SECRET_ACCESS_KEY=<AWS_SECRET_ACCESS_KEY> \
  xray_classifier
```

---

## ğŸ“ Project Structure

```text
project-root/
â”‚
â”œâ”€â”€ flowchart/                             # contain flowchart of project
â”œâ”€â”€ images/                                # contain images of project
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ start_up.sh                        # contains steps for deployment
â”‚
â”œâ”€â”€ xray/
    â””â”€â”€ components/
           â”œâ”€â”€ data_ingestion.py           # Downloads raw X-ray images
           â”œâ”€â”€ data_transformation.py      # Transforms & DataLoaders
           â”œâ”€â”€ model_trainer.py            # Training logic
           â”œâ”€â”€ model_evaluation.py         # Model evaluation
           â””â”€â”€ model_pusher.py             # BentoML model push
    â””â”€â”€ cloud_storage/
           â””â”€â”€ s3_operations.py            # AWS S3 download/upload utilities
    â””â”€â”€ entity/
          â””â”€â”€ artifact_entity.py           # Artifact dataclasses
    â””â”€â”€ml/
          â””â”€â”€ model/
                â””â”€â”€ arch.py                # CNN architecture ONLY
                â””â”€â”€ model_service.py       # CNN architecture ONLY
    â””â”€â”€constants/
          â””â”€â”€ __init__.py                  # Project-wide constants
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ experiments.ipynb                  # Research & experimentation
â”‚
â”œâ”€â”€ xray_model.pth                         # Best saved model
â”‚
â”œâ”€â”€ xray_model_last.pth                    # last saved model
â”‚
â”œâ”€â”€ app.py                                 # Inference entry-point
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ bentofile.yaml
â””â”€â”€ README.md
```

---

## ğŸ§¬ Dataset

* Source: **Kaggle Chest X-Ray Dataset**
* Classes Used:

  * `NORMAL`
  * `PNEUMONIA`
* Dataset Structure:

```text
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ NORMAL/
â”‚   â””â”€â”€ PNEUMONIA/
â””â”€â”€ test/
    â”œâ”€â”€ NORMAL/
    â””â”€â”€ PNEUMONIA/
```

> âš ï¸ The `COVID` class was **explicitly removed** to enforce **binary classification**.

---

## ğŸ”„ Data Pipeline

### ğŸ”¹ Data Ingestion

* Downloads raw image data from **AWS S3** (or local fallback)
* Stores versioned artifacts

### ğŸ”¹ Data Transformation

Applied **only on training data**:

* Resize â†’ RandomResizedCrop
* Horizontal Flip
* Light Affine Transformations (optional)
* Normalization (ImageNet stats)

Test data uses **deterministic transforms only**.

---

## ğŸ— Model Architecture

* Custom CNN (defined in `ml/model/arch.py`)
* Convolution + BatchNorm + ReLU blocks
* MaxPooling for spatial reduction
* Adaptive Average Pooling
* Fully Connected classifier

```text
Input (3Ã—224Ã—224)
â†’ Conv Blocks
â†’ AdaptiveAvgPool
â†’ Linear (2 classes)
```

---

## ğŸ‹ï¸ Training Strategy

* Loss Function: `CrossEntropyLoss`
* Optimizer: `Adam`
* Scheduler: `ReduceLROnPlateau`
* Metric: Validation Accuracy
* Checkpointing: **Best model only**

```python
torch.save(model.state_dict(), "xray_model.pth")
```

âœ” Ensures the **best-performing model**, not the last epoch, is saved.

---

## ğŸ“Š Evaluation

* Accuracy-based evaluation
* Separate evaluation pipeline
* Metrics stored as artifacts

---

## ğŸ“¦ Artifacts & Entities

Dataclasses are used to track pipeline outputs:

* `DataIngestionArtifact`
* `DataTransformationArtifact`
* `ModelTrainerArtifact`
* `ModelEvaluationArtifact`
* `ModelPusherArtifact`

This enables **traceability**, **debugging**, and **pipeline reproducibility**.

---

## ğŸš€ Deployment (BentoML Ready)

* Trained model pushed using **BentoML**
* Service name and model name configurable
* Ready for Docker & AWS ECR

---

## ğŸ–¥ Inference

Inference pipeline:

1. Load trained model
2. Apply test-time transforms
3. Perform prediction
4. Map output â†’ class label

```python
PREDICTION_LABEL = {0: "NORMAL", 1: "PNEUMONIA"}
```

---

## âš™ï¸ Configuration Highlights

All constants are centralized:

* Batch size
* Image size
* Normalization stats
* Epochs
* Learning rate schedule

This avoids **hardcoding** and supports **easy experimentation**.

---

## âœ… Final Outcome

* âœ” Successfully trained a binary pneumonia detection model
* âœ” Clean separation of research vs production code
* âœ” Scalable, maintainable ML architecture
* âœ” Ready for real-world deployment

---

## ğŸ§ª Future Improvements

* Impove accuracy of model
* Extend to multi-label classification

---

## ğŸ¤ Author Note

This project demonstrates **end-to-end ML system design**, not just model training aligning closely with **industry and MLOps standards**.
This project was built under the guidance of PWSkills team. 

If you're reviewing this as part of an interview or production audit â€” this codebase is designed to pass both âœ…

